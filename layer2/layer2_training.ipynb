{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
    "from scipy.special import softmax\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_SEED = 42 #for reproducibility\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Config\n",
    "MODEL_NAME = \"answerdotai/ModernBERT-base\"\n",
    "MAX_LENGTH = 256  # Increased to fit both texts\n",
    "HIDDEN_DIM = 256\n",
    "DROPOUT = 0.3\n",
    "\n",
    "# Embedding dimensions\n",
    "INTENT_EMBED_DIM = 32\n",
    "STAGE_EMBED_DIM = 32\n",
    "INTENT_HISTORY_LEN = 3\n",
    "\n",
    "# Training configuration\n",
    "BATCH_SIZE = 32\n",
    "EVAL_BATCH_SIZE = 64\n",
    "LEARNING_RATE = 5e-4\n",
    "NUM_EPOCHS = 10\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_RATIO = 0.1\n",
    "PATIENCE = 3  # Early stopping patience\n",
    "\n",
    "# Label mappings for Layer 2 multi-class classification\n",
    "id2label = {\n",
    "    0: \"answer_submission\",\n",
    "    1: \"clarification_request\",\n",
    "    2: \"process_inquiry\",\n",
    "    3: \"challenge_assessment\",\n",
    "    4: \"off_topic\",\n",
    "    5: \"small_talk\"\n",
    "}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "NUM_LABELS = len(id2label)\n",
    "\n",
    "# Stage mappings (4 stages)\n",
    "stage2id = {\n",
    "    \"opening\": 0,\n",
    "    \"technical_depth\": 1,\n",
    "    \"challenge\": 2,\n",
    "    \"closing\": 3\n",
    "}\n",
    "id2stage = {v: k for k, v in stage2id.items()}\n",
    "NUM_STAGES = len(stage2id)\n",
    "\n",
    "# Intent padding index (for -1 values in history)\n",
    "INTENT_PADDING_IDX = NUM_LABELS  # Index 6 for padding\n",
    "\n",
    "print(f\"Number of intent classes: {NUM_LABELS}\")\n",
    "print(f\"Intent labels: {list(id2label.values())}\")\n",
    "print(f\"Number of stages: {NUM_STAGES}\")\n",
    "print(f\"Stages: {list(stage2id.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected CSV columns: current_query, prev_system_msg, prev_intents, interview_stage, label\n",
    "df = pd.read_csv('layer2_contextual_data.csv')\n",
    "\n",
    "# Clean data\n",
    "df['label'] = pd.to_numeric(df['label'], errors='coerce')\n",
    "df = df.dropna(subset=['label'])\n",
    "df['label'] = df['label'].astype(int)\n",
    "df = df[df['label'].isin([0, 1, 2, 3, 4, 5])].copy()\n",
    "\n",
    "# Parse prev_intents from string to list\n",
    "def parse_intents(intent_str):\n",
    "    \"\"\"Convert string like '[-1, -1, 0]' to list of ints\"\"\"\n",
    "    try:\n",
    "        return ast.literal_eval(intent_str)\n",
    "    except:\n",
    "        return [-1, -1, -1]\n",
    "\n",
    "df['prev_intents_parsed'] = df['prev_intents'].apply(parse_intents)\n",
    "\n",
    "# Validate stages\n",
    "df = df[df['interview_stage'].isin(stage2id.keys())].copy()\n",
    "\n",
    "print(f\"Loaded {len(df)} samples\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts().sort_index())\n",
    "print(f\"\\nStage distribution:\")\n",
    "print(df['interview_stage'].value_counts())\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split: 70% train, 30% val/test\n",
    "train_df, temp_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.30, \n",
    "    random_state=RANDOM_SEED, \n",
    "    stratify=df['label']\n",
    ")\n",
    "\n",
    "# Second split: 15% val, 15% test (50-50 split of the 30%)\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, \n",
    "    test_size=0.50, \n",
    "    random_state=RANDOM_SEED, \n",
    "    stratify=temp_df['label']\n",
    ")\n",
    "\n",
    "# Reset indices\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "print(f\"Train size: {len(train_df)} ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"Val size: {len(val_df)} ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"Test size: {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "splits = [\n",
    "    ('Train', train_df),\n",
    "    ('Validation', val_df),\n",
    "    ('Test', test_df)\n",
    "]\n",
    "\n",
    "colors_intent = ['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4', '#ffeaa7', '#dfe6e9']\n",
    "colors_stage = ['#a29bfe', '#fd79a8', '#00b894', '#e17055']\n",
    "class_names = [id2label[i] for i in range(NUM_LABELS)]\n",
    "stage_names = list(stage2id.keys())\n",
    "\n",
    "# Row 1: Intent distribution\n",
    "for idx, (split_name, split_df) in enumerate(splits):\n",
    "    counts = split_df['label'].value_counts().sort_index()\n",
    "    percentages = (counts / len(split_df) * 100)\n",
    "    \n",
    "    bars = axes[0, idx].bar(range(NUM_LABELS), counts.values, color=colors_intent)\n",
    "    axes[0, idx].set_title(f'{split_name} - Intent Distribution (n={len(split_df):,})', fontsize=12, fontweight='bold')\n",
    "    axes[0, idx].set_ylabel('Count')\n",
    "    axes[0, idx].set_xticks(range(NUM_LABELS))\n",
    "    axes[0, idx].set_xticklabels(class_names, rotation=45, ha='right', fontsize=9)\n",
    "    \n",
    "    for bar, count, pct in zip(bars, counts.values, percentages.values):\n",
    "        label = f'{count:,}\\n({pct:.1f}%)'\n",
    "        axes[0, idx].text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
    "                          label, ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Row 2: Stage distribution\n",
    "for idx, (split_name, split_df) in enumerate(splits):\n",
    "    counts = split_df['interview_stage'].value_counts()\n",
    "    # Reorder to match stage2id\n",
    "    counts = counts.reindex(stage_names)\n",
    "    percentages = (counts / len(split_df) * 100)\n",
    "    \n",
    "    bars = axes[1, idx].bar(range(NUM_STAGES), counts.values, color=colors_stage)\n",
    "    axes[1, idx].set_title(f'{split_name} - Stage Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[1, idx].set_ylabel('Count')\n",
    "    axes[1, idx].set_xticks(range(NUM_STAGES))\n",
    "    axes[1, idx].set_xticklabels(stage_names, rotation=45, ha='right', fontsize=9)\n",
    "    \n",
    "    for bar, count, pct in zip(bars, counts.values, percentages.values):\n",
    "        label = f'{count:,}\\n({pct:.1f}%)'\n",
    "        axes[1, idx].text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
    "                          label, ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.suptitle(f'Data Distribution Across Splits - Total: {len(df):,} samples', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualIntentDataset(TorchDataset):\n",
    "    \"\"\"Dataset for combined context-aware intent classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, df, tokenizer, max_length=256):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Combine texts with separator: [prev_msg] [SEP] [current_query]\n",
    "        prev_msg = str(row['prev_system_msg']) if pd.notna(row['prev_system_msg']) else \"\"\n",
    "        current_query = str(row['current_query'])\n",
    "        combined_text = f\"{prev_msg} {self.tokenizer.sep_token} {current_query}\"\n",
    "        \n",
    "        # Tokenize combined text\n",
    "        encoding = self.tokenizer(\n",
    "            combined_text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Process intent history (convert -1 to padding index)\n",
    "        prev_intents = row['prev_intents_parsed']\n",
    "        prev_intents_tensor = torch.tensor([\n",
    "            idx if idx != -1 else INTENT_PADDING_IDX \n",
    "            for idx in prev_intents\n",
    "        ], dtype=torch.long)\n",
    "        \n",
    "        # Convert stage to integer\n",
    "        stage = stage2id[row['interview_stage']]\n",
    "        \n",
    "        # Label\n",
    "        label = row['label']\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'prev_intents': prev_intents_tensor,\n",
    "            'interview_stage': torch.tensor(stage, dtype=torch.long),\n",
    "            'labels': torch.tensor(label, dtype=torch.long),\n",
    "            # Store metadata for stratified evaluation\n",
    "            'stage_name': row['interview_stage'],\n",
    "            'is_first_turn': all(i == -1 for i in prev_intents)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextAwareLayer2Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Context-aware intent classifier that combines:\n",
    "    - Text encoding (ModernBERT with [prev_msg] [SEP] [current_query])\n",
    "    - Intent history embeddings (3 positions, right-aligned)\n",
    "    - Interview stage embedding\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"answerdotai/ModernBERT-base\",\n",
    "        num_labels: int = 6,\n",
    "        num_stages: int = 4,\n",
    "        intent_history_len: int = 3,\n",
    "        intent_embed_dim: int = 32,\n",
    "        stage_embed_dim: int = 32,\n",
    "        hidden_dim: int = 256,\n",
    "        dropout: float = 0.3,\n",
    "        intent_padding_idx: int = 6\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_labels = num_labels\n",
    "        self.intent_history_len = intent_history_len\n",
    "        \n",
    "        # ModernBERT encoder for text\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.bert_hidden_size = self.bert.config.hidden_size  # 768\n",
    "        \n",
    "        # Freeze BERT parameters\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Intent history embeddings\n",
    "        # num_embeddings = num_labels + 1 (for padding)\n",
    "        self.intent_embeddings = nn.Embedding(\n",
    "            num_embeddings=num_labels + 1,\n",
    "            embedding_dim=intent_embed_dim,\n",
    "            padding_idx=intent_padding_idx\n",
    "        )\n",
    "        \n",
    "        # Stage embedding\n",
    "        self.stage_embedding = nn.Embedding(\n",
    "            num_embeddings=num_stages,\n",
    "            embedding_dim=stage_embed_dim\n",
    "        )\n",
    "        \n",
    "        # Calculate combined dimension\n",
    "        # BERT (768) + intent history (3 * 32 = 96) + stage (32) = 896\n",
    "        combined_dim = self.bert_hidden_size + (intent_history_len * intent_embed_dim) + stage_embed_dim\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(combined_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_labels)\n",
    "        )\n",
    "        \n",
    "        # Store config for saving/loading\n",
    "        self.config = {\n",
    "            'model_name': model_name,\n",
    "            'num_labels': num_labels,\n",
    "            'num_stages': num_stages,\n",
    "            'intent_history_len': intent_history_len,\n",
    "            'intent_embed_dim': intent_embed_dim,\n",
    "            'stage_embed_dim': stage_embed_dim,\n",
    "            'hidden_dim': hidden_dim,\n",
    "            'dropout': dropout,\n",
    "            'intent_padding_idx': intent_padding_idx,\n",
    "            'combined_dim': combined_dim\n",
    "        }\n",
    "        \n",
    "        print(f\"Model architecture:\")\n",
    "        print(f\"  BERT hidden size: {self.bert_hidden_size}\")\n",
    "        print(f\"  Intent history: {intent_history_len} x {intent_embed_dim} = {intent_history_len * intent_embed_dim}\")\n",
    "        print(f\"  Stage embedding: {stage_embed_dim}\")\n",
    "        print(f\"  Combined dimension: {combined_dim}\")\n",
    "        print(f\"  Output classes: {num_labels}\")\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        prev_intents: torch.Tensor,\n",
    "        interview_stage: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Tokenized text [batch_size, seq_len]\n",
    "            attention_mask: Attention mask [batch_size, seq_len]\n",
    "            prev_intents: Intent history [batch_size, 3]\n",
    "            interview_stage: Stage indices [batch_size]\n",
    "        \n",
    "        Returns:\n",
    "            logits: Classification logits [batch_size, num_labels]\n",
    "        \"\"\"\n",
    "        # 1. Encode text through BERT\n",
    "        bert_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        # Use [CLS] token embedding\n",
    "        text_embedding = bert_output.last_hidden_state[:, 0, :]  # [batch_size, 768]\n",
    "        \n",
    "        # 2. Embed intent history\n",
    "        intent_emb = self.intent_embeddings(prev_intents)  # [batch_size, 3, 32]\n",
    "        intent_emb = intent_emb.view(intent_emb.size(0), -1)  # [batch_size, 96]\n",
    "        \n",
    "        # 3. Embed stage\n",
    "        stage_emb = self.stage_embedding(interview_stage)  # [batch_size, 32]\n",
    "        \n",
    "        # 4. Concatenate all embeddings\n",
    "        combined = torch.cat([text_embedding, intent_emb, stage_emb], dim=-1)  # [batch_size, 896]\n",
    "        \n",
    "        # 5. Classification\n",
    "        logits = self.classifier(combined)  # [batch_size, num_labels]\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count trainable and frozen parameters.\"\"\"\n",
    "        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        frozen = sum(p.numel() for p in self.parameters() if not p.requires_grad)\n",
    "        return trainable, frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model = ContextAwareLayer2Classifier(\n",
    "    model_name=MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    num_stages=NUM_STAGES,\n",
    "    intent_history_len=INTENT_HISTORY_LEN,\n",
    "    intent_embed_dim=INTENT_EMBED_DIM,\n",
    "    stage_embed_dim=STAGE_EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    dropout=DROPOUT,\n",
    "    intent_padding_idx=INTENT_PADDING_IDX\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "trainable, frozen = model.count_parameters()\n",
    "print(f\"\\nTrainable parameters: {trainable:,}\")\n",
    "print(f\"Frozen parameters: {frozen:,}\")\n",
    "print(f\"Total parameters: {trainable + frozen:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ContextualIntentDataset(train_df, tokenizer, MAX_LENGTH)\n",
    "val_dataset = ContextualIntentDataset(val_df, tokenizer, MAX_LENGTH)\n",
    "test_dataset = ContextualIntentDataset(test_df, tokenizer, MAX_LENGTH)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle metadata.\"\"\"\n",
    "    return {\n",
    "        'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
    "        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
    "        'prev_intents': torch.stack([item['prev_intents'] for item in batch]),\n",
    "        'interview_stage': torch.stack([item['interview_stage'] for item in batch]),\n",
    "        'labels': torch.stack([item['labels'] for item in batch]),\n",
    "        'stage_name': [item['stage_name'] for item in batch],\n",
    "        'is_first_turn': [item['is_first_turn'] for item in batch]\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=EVAL_BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=EVAL_BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(all_labels, all_preds, all_probs, all_stages, all_first_turn):\n",
    "    \"\"\"\n",
    "    Compute comprehensive metrics including stratified evaluations.\n",
    "    \n",
    "    Args:\n",
    "        all_labels: True labels\n",
    "        all_preds: Predicted labels\n",
    "        all_probs: Prediction probabilities\n",
    "        all_stages: Stage names for each sample\n",
    "        all_first_turn: Boolean for first turn samples\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of metrics\n",
    "    \"\"\"\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probs = np.array(all_probs)\n",
    "    all_stages = np.array(all_stages)\n",
    "    all_first_turn = np.array(all_first_turn)\n",
    "    \n",
    "    confidence = np.max(all_probs, axis=-1)\n",
    "    \n",
    "    # Per-class metrics\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average=None, labels=list(range(NUM_LABELS)), zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Macro averages\n",
    "    macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='macro', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Per-class AUC\n",
    "    per_class_auc = []\n",
    "    for i in range(NUM_LABELS):\n",
    "        binary_labels = (all_labels == i).astype(int)\n",
    "        if binary_labels.sum() > 0 and binary_labels.sum() < len(binary_labels):\n",
    "            auc = roc_auc_score(binary_labels, all_probs[:, i])\n",
    "        else:\n",
    "            auc = 0.0\n",
    "        per_class_auc.append(auc)\n",
    "    \n",
    "    # Confidence stats\n",
    "    correct_mask = all_preds == all_labels\n",
    "    \n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(all_labels, all_preds),\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"macro_precision\": macro_precision,\n",
    "        \"macro_recall\": macro_recall,\n",
    "        \"macro_auc\": np.mean(per_class_auc),\n",
    "        \"confidence_mean\": confidence.mean(),\n",
    "        \"confidence_correct\": confidence[correct_mask].mean() if correct_mask.any() else 0.0,\n",
    "        \"confidence_wrong\": confidence[~correct_mask].mean() if (~correct_mask).any() else 0.0,\n",
    "    }\n",
    "    \n",
    "    # Per-class metrics\n",
    "    for i, label_name in id2label.items():\n",
    "        metrics[f\"f1_{label_name}\"] = f1[i]\n",
    "        metrics[f\"precision_{label_name}\"] = precision[i]\n",
    "        metrics[f\"recall_{label_name}\"] = recall[i]\n",
    "        metrics[f\"auc_{label_name}\"] = per_class_auc[i]\n",
    "    \n",
    "   # 1. Accuracy by interview stage\n",
    "    for stage_name in stage2id.keys():\n",
    "        stage_mask = all_stages == stage_name\n",
    "        if stage_mask.sum() > 0:\n",
    "            stage_acc = accuracy_score(all_labels[stage_mask], all_preds[stage_mask])\n",
    "            metrics[f\"accuracy_stage_{stage_name}\"] = stage_acc\n",
    "        else:\n",
    "            metrics[f\"accuracy_stage_{stage_name}\"] = 0.0\n",
    "    \n",
    "    # 2. Accuracy for first turn vs later turns\n",
    "    first_turn_mask = all_first_turn\n",
    "    later_turn_mask = ~all_first_turn\n",
    "    \n",
    "    if first_turn_mask.sum() > 0:\n",
    "        metrics[\"accuracy_first_turn\"] = accuracy_score(\n",
    "            all_labels[first_turn_mask], all_preds[first_turn_mask]\n",
    "        )\n",
    "    else:\n",
    "        metrics[\"accuracy_first_turn\"] = 0.0\n",
    "    \n",
    "    if later_turn_mask.sum() > 0:\n",
    "        metrics[\"accuracy_later_turns\"] = accuracy_score(\n",
    "            all_labels[later_turn_mask], all_preds[later_turn_mask]\n",
    "        )\n",
    "    else:\n",
    "        metrics[\"accuracy_later_turns\"] = 0.0\n",
    "    \n",
    "    # Sample counts for reference\n",
    "    metrics[\"n_first_turn\"] = int(first_turn_mask.sum())\n",
    "    metrics[\"n_later_turns\"] = int(later_turn_mask.sum())\n",
    "    for stage_name in stage2id.keys():\n",
    "        metrics[f\"n_stage_{stage_name}\"] = int((all_stages == stage_name).sum())\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"Evaluate model and return metrics.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_stages = []\n",
    "    all_first_turn = []\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            prev_intents = batch['prev_intents'].to(device)\n",
    "            interview_stage = batch['interview_stage'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask, prev_intents, interview_stage)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "            preds = np.argmax(probs, axis=-1)\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds)\n",
    "            all_probs.extend(probs)\n",
    "            all_stages.extend(batch['stage_name'])\n",
    "            all_first_turn.extend(batch['is_first_turn'])\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    metrics = compute_metrics(all_labels, all_preds, all_probs, all_stages, all_first_turn)\n",
    "    metrics['loss'] = avg_loss\n",
    "    \n",
    "    return metrics, all_labels, all_preds, all_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer (only trainable parameters)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_accuracy': [],\n",
    "    'val_macro_f1': []\n",
    "}\n",
    "\n",
    "best_val_f1 = 0.0\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "print(f\"Starting training for {NUM_EPOCHS} epochs...\")\n",
    "print(f\"Total steps: {total_steps}, Warmup steps: {warmup_steps}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        prev_intents = batch['prev_intents'].to(device)\n",
    "        interview_stage = batch['interview_stage'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(input_ids, attention_mask, prev_intents, interview_stage)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    \n",
    "    # Validation\n",
    "    val_metrics, _, _, _ = evaluate(model, val_loader, device)\n",
    "    \n",
    "    # Update history\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    history['val_loss'].append(val_metrics['loss'])\n",
    "    history['val_accuracy'].append(val_metrics['accuracy'])\n",
    "    history['val_macro_f1'].append(val_metrics['macro_f1'])\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_metrics['loss']:.4f}\")\n",
    "    print(f\"  Val Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Val Macro F1: {val_metrics['macro_f1']:.4f}\")\n",
    "    print(f\"  Val Accuracy (First Turn): {val_metrics['accuracy_first_turn']:.4f}\")\n",
    "    print(f\"  Val Accuracy (Later Turns): {val_metrics['accuracy_later_turns']:.4f}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if val_metrics['macro_f1'] > best_val_f1:\n",
    "        best_val_f1 = val_metrics['macro_f1']\n",
    "        patience_counter = 0\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        print(f\"  ✓ New best model! (Macro F1: {best_val_f1:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  No improvement. Patience: {patience_counter}/{PATIENCE}\")\n",
    "    \n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f\"\\nEarly stopping triggered after {epoch+1} epochs.\")\n",
    "        break\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nTraining complete! Best Macro F1: {best_val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD BEST MODEL AND SAVE\n",
    "# ============================================================\n",
    "\n",
    "# Load best model state\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "# Save model\n",
    "import os\n",
    "save_dir = \"./layer2_contextual_model\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save model state and config\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': model.config,\n",
    "    'id2label': id2label,\n",
    "    'label2id': label2id,\n",
    "    'stage2id': stage2id,\n",
    "    'id2stage': id2stage\n",
    "}, os.path.join(save_dir, 'model.pt'))\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(f\"✓ Model saved to {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAINING HISTORY VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(epochs_range, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "axes[0].plot(epochs_range, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(epochs_range, history['val_accuracy'], 'g-', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Validation Accuracy', fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Macro F1\n",
    "axes[2].plot(epochs_range, history['val_macro_f1'], 'purple', linewidth=2)\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Macro F1')\n",
    "axes[2].set_title('Validation Macro F1', fontweight='bold')\n",
    "axes[2].axhline(y=best_val_f1, color='red', linestyle='--', label=f'Best: {best_val_f1:.4f}')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Training History', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEST SET EVALUATION\n",
    "# ============================================================\n",
    "\n",
    "test_metrics, test_labels, test_preds, test_probs = evaluate(model, test_loader, device)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nOverall Metrics:\")\n",
    "print(f\"  Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Macro F1: {test_metrics['macro_f1']:.4f}\")\n",
    "print(f\"  Macro Precision: {test_metrics['macro_precision']:.4f}\")\n",
    "print(f\"  Macro Recall: {test_metrics['macro_recall']:.4f}\")\n",
    "print(f\"  Macro AUC: {test_metrics['macro_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\nConfidence Statistics:\")\n",
    "print(f\"  Mean Confidence: {test_metrics['confidence_mean']:.4f}\")\n",
    "print(f\"  Confidence (Correct): {test_metrics['confidence_correct']:.4f}\")\n",
    "print(f\"  Confidence (Wrong): {test_metrics['confidence_wrong']:.4f}\")\n",
    "\n",
    "print(f\"\\nStratified by Turn:\")\n",
    "print(f\"  First Turn Accuracy: {test_metrics['accuracy_first_turn']:.4f} (n={test_metrics['n_first_turn']})\")\n",
    "print(f\"  Later Turns Accuracy: {test_metrics['accuracy_later_turns']:.4f} (n={test_metrics['n_later_turns']})\")\n",
    "\n",
    "print(f\"\\nStratified by Stage:\")\n",
    "for stage_name in stage2id.keys():\n",
    "    acc = test_metrics[f'accuracy_stage_{stage_name}']\n",
    "    n = test_metrics[f'n_stage_{stage_name}']\n",
    "    print(f\"  {stage_name}: {acc:.4f} (n={n})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PER-CLASS METRICS VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "class_names = [id2label[i] for i in range(NUM_LABELS)]\n",
    "x = np.arange(NUM_LABELS)\n",
    "bar_width = 0.6\n",
    "colors_intent = ['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4', '#ffeaa7', '#dfe6e9']\n",
    "\n",
    "# F1 Scores\n",
    "f1_scores = [test_metrics[f\"f1_{id2label[i]}\"] for i in range(NUM_LABELS)]\n",
    "bars = axes[0].bar(x, f1_scores, bar_width, color=colors_intent)\n",
    "axes[0].set_title('F1 Score per Class', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('F1 Score')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(class_names, rotation=45, ha='right', fontsize=9)\n",
    "axes[0].set_ylim(0, 1.15)\n",
    "axes[0].axhline(y=test_metrics[\"macro_f1\"], color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Macro F1: {test_metrics[\"macro_f1\"]:.3f}')\n",
    "axes[0].legend(loc='upper right', framealpha=0.9)\n",
    "for bar, score in zip(bars, f1_scores):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, f'{score:.3f}', ha='center', fontsize=9)\n",
    "\n",
    "# Precision Scores\n",
    "precision_scores = [test_metrics[f\"precision_{id2label[i]}\"] for i in range(NUM_LABELS)]\n",
    "bars = axes[1].bar(x, precision_scores, bar_width, color=colors_intent)\n",
    "axes[1].set_title('Precision per Class', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(class_names, rotation=45, ha='right', fontsize=9)\n",
    "axes[1].set_ylim(0, 1.15)\n",
    "axes[1].axhline(y=test_metrics[\"macro_precision\"], color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Macro: {test_metrics[\"macro_precision\"]:.3f}')\n",
    "axes[1].legend(loc='upper right', framealpha=0.9)\n",
    "for bar, score in zip(bars, precision_scores):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, f'{score:.3f}', ha='center', fontsize=9)\n",
    "\n",
    "# Recall Scores\n",
    "recall_scores = [test_metrics[f\"recall_{id2label[i]}\"] for i in range(NUM_LABELS)]\n",
    "bars = axes[2].bar(x, recall_scores, bar_width, color=colors_intent)\n",
    "axes[2].set_title('Recall per Class', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylabel('Recall')\n",
    "axes[2].set_xticks(x)\n",
    "axes[2].set_xticklabels(class_names, rotation=45, ha='right', fontsize=9)\n",
    "axes[2].set_ylim(0, 1.15)\n",
    "axes[2].axhline(y=test_metrics[\"macro_recall\"], color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Macro: {test_metrics[\"macro_recall\"]:.3f}')\n",
    "axes[2].legend(loc='upper right', framealpha=0.9)\n",
    "for bar, score in zip(bars, recall_scores):\n",
    "    axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, f'{score:.3f}', ha='center', fontsize=9)\n",
    "\n",
    "plt.suptitle('Per-Class Classification Metrics on Test Set', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STRATIFIED METRICS VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# 1. Accuracy by Interview Stage\n",
    "stage_names = list(stage2id.keys())\n",
    "stage_accs = [test_metrics[f'accuracy_stage_{s}'] for s in stage_names]\n",
    "stage_counts = [test_metrics[f'n_stage_{s}'] for s in stage_names]\n",
    "colors_stage = ['#a29bfe', '#fd79a8', '#00b894', '#e17055']\n",
    "\n",
    "bars = axes[0].bar(range(len(stage_names)), stage_accs, color=colors_stage)\n",
    "axes[0].set_title('Accuracy by Interview Stage', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_xticks(range(len(stage_names)))\n",
    "axes[0].set_xticklabels(stage_names, rotation=45, ha='right', fontsize=10)\n",
    "axes[0].set_ylim(0, 1.15)\n",
    "axes[0].axhline(y=test_metrics['accuracy'], color='red', linestyle='--', linewidth=2,\n",
    "                label=f'Overall: {test_metrics[\"accuracy\"]:.3f}')\n",
    "axes[0].legend(loc='upper right')\n",
    "\n",
    "for bar, acc, count in zip(bars, stage_accs, stage_counts):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                 f'{acc:.3f}\\n(n={count})', ha='center', fontsize=9)\n",
    "\n",
    "# 2. Accuracy by Turn Type (First vs Later)\n",
    "turn_types = ['First Turn', 'Later Turns']\n",
    "turn_accs = [test_metrics['accuracy_first_turn'], test_metrics['accuracy_later_turns']]\n",
    "turn_counts = [test_metrics['n_first_turn'], test_metrics['n_later_turns']]\n",
    "colors_turn = ['#74b9ff', '#55efc4']\n",
    "\n",
    "bars = axes[1].bar(range(2), turn_accs, color=colors_turn)\n",
    "axes[1].set_title('Accuracy: First Turn vs Later Turns', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_xticks(range(2))\n",
    "axes[1].set_xticklabels(turn_types, fontsize=10)\n",
    "axes[1].set_ylim(0, 1.15)\n",
    "axes[1].axhline(y=test_metrics['accuracy'], color='red', linestyle='--', linewidth=2,\n",
    "                label=f'Overall: {test_metrics[\"accuracy\"]:.3f}')\n",
    "axes[1].legend(loc='upper right')\n",
    "\n",
    "for bar, acc, count in zip(bars, turn_accs, turn_counts):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                 f'{acc:.3f}\\n(n={count})', ha='center', fontsize=10)\n",
    "\n",
    "plt.suptitle('Stratified Accuracy Analysis', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFUSION MATRIX\n",
    "# ============================================================\n",
    "\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "class_names = [id2label[i] for i in range(NUM_LABELS)]\n",
    "\n",
    "# Raw counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title('Confusion Matrix (Counts)', fontweight='bold')\n",
    "\n",
    "# Normalized (percentages)\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', ax=axes[1],\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_title('Confusion Matrix (Normalized)', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Test Set Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PER-CLASS AUC VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "auc_scores = [test_metrics[f'auc_{id2label[i]}'] for i in range(NUM_LABELS)]\n",
    "\n",
    "bars = ax.barh(range(NUM_LABELS), auc_scores, color=colors_intent)\n",
    "ax.set_yticks(range(NUM_LABELS))\n",
    "ax.set_yticklabels(class_names)\n",
    "ax.set_xlabel('AUC Score')\n",
    "ax.set_title('Per-Class AUC (One-vs-Rest)', fontsize=12, fontweight='bold')\n",
    "ax.set_xlim(0, 1.1)\n",
    "ax.axvline(x=test_metrics['macro_auc'], color='red', linestyle='--', linewidth=2,\n",
    "           label=f'Macro AUC: {test_metrics[\"macro_auc\"]:.3f}')\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "for bar, score in zip(bars, auc_scores):\n",
    "    ax.text(score + 0.02, bar.get_y() + bar.get_height()/2, f'{score:.3f}', \n",
    "            va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INTERACTIVE INFERENCE\n",
    "# ============================================================\n",
    "\n",
    "def predict(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    current_query: str,\n",
    "    prev_system_msg: str = \"\",\n",
    "    prev_intents: list = None,\n",
    "    interview_stage: str = \"opening\",\n",
    "    confidence_threshold: float = 0.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Make a prediction with full context.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        tokenizer: Tokenizer\n",
    "        current_query: Current user query\n",
    "        prev_system_msg: Previous interviewer message\n",
    "        prev_intents: List of 3 previous intent IDs (right-aligned, -1 for padding)\n",
    "        interview_stage: One of 'opening', 'technical_depth', 'challenge', 'closing'\n",
    "        confidence_threshold: Threshold for flagging low confidence\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with prediction results\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Handle defaults\n",
    "    if prev_intents is None:\n",
    "        prev_intents = [-1, -1, -1]\n",
    "    \n",
    "    # Combine texts\n",
    "    combined_text = f\"{prev_system_msg} {tokenizer.sep_token} {current_query}\"\n",
    "    \n",
    "    # Tokenize\n",
    "    encoding = tokenizer(\n",
    "        combined_text,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Prepare inputs\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    prev_intents_tensor = torch.tensor([[\n",
    "        idx if idx != -1 else INTENT_PADDING_IDX \n",
    "        for idx in prev_intents\n",
    "    ]], dtype=torch.long).to(device)\n",
    "    \n",
    "    stage_tensor = torch.tensor([stage2id[interview_stage]], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask, prev_intents_tensor, stage_tensor)\n",
    "        probs = torch.softmax(logits, dim=-1)[0]\n",
    "    \n",
    "    pred_class = probs.argmax().item()\n",
    "    confidence = probs[pred_class].item()\n",
    "    pred_label = id2label[pred_class]\n",
    "    \n",
    "    # Determine status\n",
    "    if confidence >= confidence_threshold:\n",
    "        status = f\"ACCEPTED - {pred_label}\"\n",
    "    else:\n",
    "        status = f\"FLAGGED (low confidence) - Predicted: {pred_label}\"\n",
    "    \n",
    "    return {\n",
    "        'prediction': pred_label,\n",
    "        'prediction_id': pred_class,\n",
    "        'confidence': confidence,\n",
    "        'status': status,\n",
    "        'all_probs': {id2label[i]: probs[i].item() for i in range(NUM_LABELS)}\n",
    "    }\n",
    "\n",
    "print(\"✓ Inference function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEST INFERENCE\n",
    "# ============================================================\n",
    "\n",
    "# Example: First turn in opening stage\n",
    "result = predict(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    current_query=\"How does this interview work?\",\n",
    "    prev_system_msg=\"Hello! Welcome to the finance interview. Let me explain the process.\",\n",
    "    prev_intents=[-1, -1, -1],  # First turn\n",
    "    interview_stage=\"opening\"\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXAMPLE 1: First turn, Opening stage\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Query: 'How does this interview work?'\")\n",
    "print(f\"Previous msg: 'Hello! Welcome to the finance interview...'\")\n",
    "print(f\"Intent history: [-1, -1, -1] (first turn)\")\n",
    "print(f\"Stage: opening\")\n",
    "print(f\"\\nResult: {result['status']}\")\n",
    "print(f\"Confidence: {result['confidence']:.1%}\")\n",
    "print(f\"\\nAll probabilities:\")\n",
    "for label, prob in sorted(result['all_probs'].items(), key=lambda x: -x[1]):\n",
    "    marker = \" <--\" if label == result['prediction'] else \"\"\n",
    "    print(f\"  {label}: {prob:.1%}{marker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Technical stage with context\n",
    "result = predict(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    current_query=\"Can you explain that?\",\n",
    "    prev_system_msg=\"Calculate the WACC using a 10% cost of equity and 5% cost of debt.\",\n",
    "    prev_intents=[-1, 0, 0],  # Two previous answers\n",
    "    interview_stage=\"technical_depth\"\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXAMPLE 2: Technical stage, asking for clarification\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Query: 'Can you explain that?'\")\n",
    "print(f\"Previous msg: 'Calculate the WACC using a 10% cost of equity...'\")\n",
    "print(f\"Intent history: [-1, 0, 0] (two answers)\")\n",
    "print(f\"Stage: technical_depth\")\n",
    "print(f\"\\nResult: {result['status']}\")\n",
    "print(f\"Confidence: {result['confidence']:.1%}\")\n",
    "print(f\"\\nAll probabilities:\")\n",
    "for label, prob in sorted(result['all_probs'].items(), key=lambda x: -x[1]):\n",
    "    marker = \" <--\" if label == result['prediction'] else \"\"\n",
    "    print(f\"  {label}: {prob:.1%}{marker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Challenge stage\n",
    "result = predict(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    current_query=\"I don't think that's correct.\",\n",
    "    prev_system_msg=\"Your calculation seems to have an error. The correct answer should be 8.5%.\",\n",
    "    prev_intents=[0, 0, 1],  # Answers then clarification\n",
    "    interview_stage=\"challenge\"\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXAMPLE 3: Challenge stage, disputing assessment\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Query: 'I don't think that's correct.'\")\n",
    "print(f\"Previous msg: 'Your calculation seems to have an error...'\")\n",
    "print(f\"Intent history: [0, 0, 1] (answers + clarification)\")\n",
    "print(f\"Stage: challenge\")\n",
    "print(f\"\\nResult: {result['status']}\")\n",
    "print(f\"Confidence: {result['confidence']:.1%}\")\n",
    "print(f\"\\nAll probabilities:\")\n",
    "for label, prob in sorted(result['all_probs'].items(), key=lambda x: -x[1]):\n",
    "    marker = \" <--\" if label == result['prediction'] else \"\"\n",
    "    print(f\"  {label}: {prob:.1%}{marker}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL LOADING UTILITY (for production use)\n",
    "# ============================================================\n",
    "\n",
    "def load_model(model_dir: str, device: torch.device):\n",
    "    \"\"\"\n",
    "    Load a saved context-aware model.\n",
    "    \n",
    "    Args:\n",
    "        model_dir: Directory containing model.pt and tokenizer\n",
    "        device: Device to load model to\n",
    "    \n",
    "    Returns:\n",
    "        model, tokenizer, config\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(os.path.join(model_dir, 'model.pt'), map_location=device)\n",
    "    config = checkpoint['config']\n",
    "    \n",
    "    # Initialize model\n",
    "    model = ContextAwareLayer2Classifier(**config)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    \n",
    "    return model, tokenizer, checkpoint\n",
    "\n",
    "print(\"✓ Model loading utility defined\")\n",
    "print(\"\\nUsage:\")\n",
    "print('  model, tokenizer, checkpoint = load_model(\"./layer2_contextual_model\", device)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
