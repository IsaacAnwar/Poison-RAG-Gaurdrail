{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
        "from transformers import (\n",
        "    AutoTokenizer, \n",
        "    AutoModel, \n",
        "    Trainer, \n",
        "    TrainingArguments,\n",
        "    EarlyStoppingCallback,\n",
        "    PreTrainedModel,\n",
        "    PretrainedConfig\n",
        ")\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "RANDOM_SEED = 42 #for reproducibility\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Config\n",
        "MODEL_NAME = \"answerdotai/ModernBERT-base\"\n",
        "MAX_LENGTH = 256  # Increased to fit both texts\n",
        "HIDDEN_DIM = 256\n",
        "DROPOUT = 0.3\n",
        "\n",
        "# Embedding dimensions\n",
        "STAGE_EMBED_DIM = 32\n",
        "\n",
        "# Training configuration\n",
        "BATCH_SIZE = 32\n",
        "EVAL_BATCH_SIZE = 64\n",
        "LEARNING_RATE = 5e-5\n",
        "NUM_EPOCHS = 12\n",
        "WEIGHT_DECAY = 0.01\n",
        "WARMUP_RATIO = 0.1\n",
        "PATIENCE = 3  # Early stopping patience\n",
        "\n",
        "# Label mappings for Layer 2 multi-class classification\n",
        "id2label = {\n",
        "    0: \"answer_submission\",\n",
        "    1: \"clarification_request\",\n",
        "    2: \"process_inquiry\",\n",
        "    3: \"challenge_assessment\",\n",
        "    4: \"off_topic\",\n",
        "    5: \"small_talk\"\n",
        "}\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "NUM_LABELS = len(id2label)\n",
        "\n",
        "# Stage mappings (4 stages)\n",
        "stage2id = {\n",
        "    \"opening\": 0,\n",
        "    \"technical_depth\": 1,\n",
        "    \"challenge\": 2,\n",
        "    \"closing\": 3\n",
        "}\n",
        "id2stage = {v: k for k, v in stage2id.items()}\n",
        "NUM_STAGES = len(stage2id)\n",
        "\n",
        "print(f\"Classes: {NUM_LABELS} intents, {NUM_STAGES} stages\")\n",
        "\n",
        "MODEL_CONFIG = {\n",
        "    'model_name': MODEL_NAME,\n",
        "    'num_labels': NUM_LABELS,\n",
        "    'num_stages': NUM_STAGES,\n",
        "    'stage_embed_dim': STAGE_EMBED_DIM,\n",
        "    'hidden_dim': HIDDEN_DIM,\n",
        "    'dropout': DROPOUT\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CSV columns: user_query, prev_agent_response, interview_stage, label\n",
        "df = pd.read_csv('layer2_contextual_data.csv')\n",
        "\n",
        "# Clean data\n",
        "df['label'] = pd.to_numeric(df['label'], errors='coerce')\n",
        "df = df.dropna(subset=['label'])\n",
        "df['label'] = df['label'].astype(int)\n",
        "df = df[df['label'].isin([0, 1, 2, 3, 4, 5])].copy()\n",
        "\n",
        "# Validate stages\n",
        "df = df[df['interview_stage'].isin(stage2id.keys())].copy()\n",
        "\n",
        "print(f\"Loaded {len(df)} samples\")\n",
        "print(f\"\\nLabel distribution:\")\n",
        "print(df['label'].value_counts().sort_index())\n",
        "print(f\"\\nStage distribution:\")\n",
        "print(df['interview_stage'].value_counts())\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First split: 70% train, 30% val/test\n",
        "train_df, temp_df = train_test_split(\n",
        "    df, \n",
        "    test_size=0.30, \n",
        "    random_state=RANDOM_SEED, \n",
        "    stratify=df['label']\n",
        ")\n",
        "\n",
        "# Second split: 15% val, 15% test (50-50 split of the 30%)\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df, \n",
        "    test_size=0.50, \n",
        "    random_state=RANDOM_SEED, \n",
        "    stratify=temp_df['label']\n",
        ")\n",
        "\n",
        "# Reset indices\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "print(f\"Train size: {len(train_df)} ({len(train_df)/len(df)*100:.1f}%)\")\n",
        "print(f\"Val size: {len(val_df)} ({len(val_df)/len(df)*100:.1f}%)\")\n",
        "print(f\"Test size: {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "splits = [\n",
        "    ('Train', train_df),\n",
        "    ('Validation', val_df),\n",
        "    ('Test', test_df)\n",
        "]\n",
        "\n",
        "colors_intent = ['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4', '#ffeaa7', '#dfe6e9']\n",
        "colors_stage = ['#a29bfe', '#fd79a8', '#00b894', '#e17055']\n",
        "class_names = [id2label[i] for i in range(NUM_LABELS)]\n",
        "stage_names = list(stage2id.keys())\n",
        "\n",
        "# Row 1: Intent distribution\n",
        "for idx, (split_name, split_df) in enumerate(splits):\n",
        "    counts = split_df['label'].value_counts().sort_index()\n",
        "    percentages = (counts / len(split_df) * 100)\n",
        "    \n",
        "    bars = axes[0, idx].bar(range(NUM_LABELS), counts.values, color=colors_intent)\n",
        "    axes[0, idx].set_title(f'{split_name} - Intent Distribution (n={len(split_df):,})', fontsize=12, fontweight='bold')\n",
        "    axes[0, idx].set_ylabel('Count')\n",
        "    axes[0, idx].set_xticks(range(NUM_LABELS))\n",
        "    axes[0, idx].set_xticklabels(class_names, rotation=45, ha='right', fontsize=9)\n",
        "    \n",
        "    for bar, count, pct in zip(bars, counts.values, percentages.values):\n",
        "        label = f'{count:,}    ({pct:.1f}%)'\n",
        "        axes[0, idx].text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
        "                          label, ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "# Row 2: Stage distribution\n",
        "for idx, (split_name, split_df) in enumerate(splits):\n",
        "    counts = split_df['interview_stage'].value_counts()\n",
        "    # Reorder to match stage2id\n",
        "    counts = counts.reindex(stage_names)\n",
        "    percentages = (counts / len(split_df) * 100)\n",
        "    \n",
        "    bars = axes[1, idx].bar(range(NUM_STAGES), counts.values, color=colors_stage)\n",
        "    axes[1, idx].set_title(f'{split_name} - Stage Distribution', fontsize=12, fontweight='bold')\n",
        "    axes[1, idx].set_ylabel('Count')\n",
        "    axes[1, idx].set_xticks(range(NUM_STAGES))\n",
        "    axes[1, idx].set_xticklabels(stage_names, rotation=45, ha='right', fontsize=9)\n",
        "    \n",
        "    for bar, count, pct in zip(bars, counts.values, percentages.values):\n",
        "        label = f'{count:,}    ({pct:.1f}%)'\n",
        "        axes[1, idx].text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
        "                          label, ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "plt.suptitle(f'Data Distribution Across Splits - Total: {len(df):,} samples', \n",
        "             fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using HuggingFace Dataset format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ContextAwareLayer2Config(PretrainedConfig):\n",
        "    \"\"\"Custom config for ContextAwareLayer2Classifier.\"\"\"\n",
        "    model_type = \"context_aware_layer2\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str = \"answerdotai/ModernBERT-base\",\n",
        "        num_labels: int = 6,\n",
        "        num_stages: int = 4,\n",
        "        stage_embed_dim: int = 32,\n",
        "        hidden_dim: int = 256,\n",
        "        dropout: float = 0.3,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.model_name = model_name\n",
        "        self.num_labels = num_labels\n",
        "        self.num_stages = num_stages\n",
        "        self.stage_embed_dim = stage_embed_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.dropout = dropout\n",
        "\n",
        "\n",
        "class ContextAwareLayer2Classifier(PreTrainedModel):\n",
        "    \"\"\"\n",
        "    Context-aware intent classifier that combines:\n",
        "    - Text encoding (ModernBERT with [prev_msg] [SEP] [current_query])\n",
        "    - Interview stage embedding\n",
        "    \n",
        "    Compatible with HuggingFace Trainer.\n",
        "    \"\"\"\n",
        "    config_class = ContextAwareLayer2Config\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        \n",
        "        self.num_labels = config.num_labels\n",
        "        \n",
        "        # ModernBERT encoder for text\n",
        "        self.bert = AutoModel.from_pretrained(config.model_name)\n",
        "        self.bert_hidden_size = self.bert.config.hidden_size  # 768\n",
        "        \n",
        "        # Freeze BERT parameters\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "        \n",
        "        # Stage embedding\n",
        "        self.stage_embedding = nn.Embedding(\n",
        "            num_embeddings=config.num_stages,\n",
        "            embedding_dim=config.stage_embed_dim\n",
        "        )\n",
        "        \n",
        "        # Calculate combined dimension\n",
        "        # BERT (768) + stage (32) = 800\n",
        "        combined_dim = self.bert_hidden_size + config.stage_embed_dim\n",
        "        \n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(combined_dim, config.hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config.dropout),\n",
        "            nn.Linear(config.hidden_dim, config.num_labels)\n",
        "        )\n",
        "    \n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: torch.Tensor,\n",
        "        interview_stage: torch.Tensor,\n",
        "        labels: torch.Tensor = None,\n",
        "        **kwargs  # Accept other kwargs from Trainer\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Forward pass compatible with HuggingFace Trainer.\n",
        "        \n",
        "        Args:\n",
        "            input_ids: Tokenized text [batch_size, seq_len]\n",
        "            attention_mask: Attention mask [batch_size, seq_len]\n",
        "            interview_stage: Stage indices [batch_size]\n",
        "            labels: Ground truth labels [batch_size] (optional)\n",
        "        \n",
        "        Returns:\n",
        "            dict with 'loss' (if labels provided) and 'logits'\n",
        "        \"\"\"\n",
        "        # 1. Encode text through BERT\n",
        "        bert_output = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        # Use [CLS] token embedding\n",
        "        text_embedding = bert_output.last_hidden_state[:, 0, :]  # [batch_size, 768]\n",
        "        \n",
        "        # 2. Embed stage\n",
        "        stage_emb = self.stage_embedding(interview_stage)  # [batch_size, 32]\n",
        "        \n",
        "        # 3. Concatenate all embeddings\n",
        "        combined = torch.cat([text_embedding, stage_emb], dim=-1)  # [batch_size, 800]\n",
        "        \n",
        "        # 4. Classification\n",
        "        logits = self.classifier(combined)  # [batch_size, num_labels]\n",
        "        \n",
        "        # 5. Compute loss if labels provided\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits, labels)\n",
        "        \n",
        "        # Return in format expected by Trainer\n",
        "        return {\n",
        "            'loss': loss,\n",
        "            'logits': logits\n",
        "        }\n",
        "    \n",
        "    def count_parameters(self):\n",
        "        \"\"\"Count trainable and frozen parameters.\"\"\"\n",
        "        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "        frozen = sum(p.numel() for p in self.parameters() if not p.requires_grad)\n",
        "        return trainable, frozen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "config = ContextAwareLayer2Config(**MODEL_CONFIG)\n",
        "model = ContextAwareLayer2Classifier(config)\n",
        "\n",
        "trainable, frozen = model.count_parameters()\n",
        "print(f\"Parameters: {trainable:,} trainable, {frozen:,} frozen\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_dataset(df):\n",
        "    \"\"\"Prepare dataframe for tokenization.\"\"\"\n",
        "    df['combined_text'] = df.apply(\n",
        "        lambda row: f\"{str(row['prev_agent_response']) if pd.notna(row['prev_agent_response']) else ''} {tokenizer.sep_token} {str(row['user_query'])}\",\n",
        "        axis=1\n",
        "    )\n",
        "    df['interview_stage'] = df['interview_stage'].map(stage2id)\n",
        "    return df[['combined_text', 'interview_stage', 'label']].copy()\n",
        "\n",
        "train_prepared = prepare_dataset(train_df)\n",
        "val_prepared = prepare_dataset(val_df)\n",
        "test_prepared = prepare_dataset(test_df)\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_prepared.reset_index(drop=True))\n",
        "val_dataset = Dataset.from_pandas(val_prepared.reset_index(drop=True))\n",
        "test_dataset = Dataset.from_pandas(test_prepared.reset_index(drop=True))\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    tokenized = tokenizer(\n",
        "        examples['combined_text'],\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding='max_length'\n",
        "    )\n",
        "    tokenized['interview_stage'] = examples['interview_stage']\n",
        "    tokenized['labels'] = examples['label']\n",
        "    return tokenized\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=['combined_text', 'label'])\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=['combined_text', 'label'])\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=['combined_text', 'label'])\n",
        "\n",
        "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'interview_stage', 'labels'])\n",
        "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'interview_stage', 'labels'])\n",
        "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'interview_stage', 'labels'])\n",
        "\n",
        "print(f\"Datasets ready: train={len(train_dataset)}, val={len(val_dataset)}, test={len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics_for_trainer(eval_pred):\n",
        "    \"\"\"\n",
        "    Compute metrics for HuggingFace Trainer.\n",
        "    \n",
        "    Args:\n",
        "        eval_pred: EvalPrediction object with predictions and label_ids\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary of metrics\n",
        "    \"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    \n",
        "    # predictions are logits, convert to probabilities and predicted classes\n",
        "    import scipy.special\n",
        "    probs = scipy.special.softmax(predictions, axis=-1)\n",
        "    preds = np.argmax(predictions, axis=-1)\n",
        "    \n",
        "    confidence = np.max(probs, axis=-1)\n",
        "    \n",
        "    # Per-class metrics\n",
        "    precision, recall, f1, support = precision_recall_fscore_support(\n",
        "        labels, preds, average=None, labels=list(range(NUM_LABELS)), zero_division=0\n",
        "    )\n",
        "    \n",
        "    # Macro averages\n",
        "    macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(\n",
        "        labels, preds, average='macro', zero_division=0\n",
        "    )\n",
        "    \n",
        "    # Per-class AUC\n",
        "    per_class_auc = []\n",
        "    for i in range(NUM_LABELS):\n",
        "        binary_labels = (labels == i).astype(int)\n",
        "        if binary_labels.sum() > 0 and binary_labels.sum() < len(binary_labels):\n",
        "            auc = roc_auc_score(binary_labels, probs[:, i])\n",
        "        else:\n",
        "            auc = 0.0\n",
        "        per_class_auc.append(auc)\n",
        "    \n",
        "    # Confidence stats\n",
        "    correct_mask = preds == labels\n",
        "    \n",
        "    metrics = {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"macro_f1\": macro_f1,\n",
        "        \"macro_precision\": macro_precision,\n",
        "        \"macro_recall\": macro_recall,\n",
        "        \"macro_auc\": np.mean(per_class_auc),\n",
        "        \"confidence_mean\": confidence.mean(),\n",
        "        \"confidence_correct\": confidence[correct_mask].mean() if correct_mask.any() else 0.0,\n",
        "        \"confidence_wrong\": confidence[~correct_mask].mean() if (~correct_mask).any() else 0.0,\n",
        "    }\n",
        "    \n",
        "    for i, label_name in id2label.items():\n",
        "        metrics[f\"f1_{label_name}\"] = f1[i]\n",
        "        metrics[f\"precision_{label_name}\"] = precision[i]\n",
        "        metrics[f\"recall_{label_name}\"] = recall[i]\n",
        "        metrics[f\"auc_{label_name}\"] = per_class_auc[i]\n",
        "    \n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_with_stages(model, dataset, df_original, batch_size=64):\n",
        "    \"\"\"\n",
        "    Evaluate model with stage-stratified metrics.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained model\n",
        "        dataset: HuggingFace Dataset\n",
        "        df_original: Original dataframe with stage_name column\n",
        "        batch_size: Evaluation batch size\n",
        "    \n",
        "    Returns:\n",
        "        metrics dict, labels, predictions, probabilities\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    all_probs = []\n",
        "    \n",
        "    # Create temporary trainer for evaluation\n",
        "    temp_trainer = Trainer(\n",
        "        model=model,\n",
        "        args=TrainingArguments(\n",
        "            output_dir='./temp',\n",
        "            per_device_eval_batch_size=batch_size,\n",
        "            report_to=[]\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    # Get predictions\n",
        "    predictions = temp_trainer.predict(dataset)\n",
        "    logits = predictions.predictions\n",
        "    labels = predictions.label_ids\n",
        "    \n",
        "    # Convert to probabilities\n",
        "    import scipy.special\n",
        "    probs = scipy.special.softmax(logits, axis=-1)\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    \n",
        "    # Compute base metrics\n",
        "    metrics = compute_metrics_for_trainer(predictions)\n",
        "    \n",
        "    # Add stage-stratified metrics\n",
        "    stage_names = df_original['interview_stage'].values\n",
        "    for stage_name in stage2id.keys():\n",
        "        stage_mask = stage_names == stage_name\n",
        "        if stage_mask.sum() > 0:\n",
        "            stage_acc = accuracy_score(labels[stage_mask], preds[stage_mask])\n",
        "            metrics[f\"accuracy_stage_{stage_name}\"] = stage_acc\n",
        "            metrics[f\"n_stage_{stage_name}\"] = int(stage_mask.sum())\n",
        "        else:\n",
        "            metrics[f\"accuracy_stage_{stage_name}\"] = 0.0\n",
        "            metrics[f\"n_stage_{stage_name}\"] = 0\n",
        "    \n",
        "    return metrics, labels, preds, probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./layer2_contextual_model\",\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"macro_f1\",\n",
        "    greater_is_better=True,\n",
        "    logging_steps=50,\n",
        "    report_to=[],\n",
        "    save_total_limit=2,\n",
        "    seed=RANDOM_SEED,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics_for_trainer,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=PATIENCE)],\n",
        ")\n",
        "\n",
        "print(f\"Training: {NUM_EPOCHS} epochs, batch_size={BATCH_SIZE}, lr={LEARNING_RATE}, early_stop={PATIENCE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_result = trainer.train()\n",
        "\n",
        "history = {'train_loss': [], 'val_loss': [], 'val_accuracy': [], 'val_macro_f1': []}\n",
        "for log in trainer.state.log_history:\n",
        "    if 'loss' in log and 'epoch' in log:\n",
        "        history['train_loss'].append(log['loss'])\n",
        "    if 'eval_loss' in log:\n",
        "        history['val_loss'].append(log['eval_loss'])\n",
        "    if 'eval_accuracy' in log:\n",
        "        history['val_accuracy'].append(log['eval_accuracy'])\n",
        "    if 'eval_macro_f1' in log:\n",
        "        history['val_macro_f1'].append(log['eval_macro_f1'])\n",
        "\n",
        "best_val_f1 = max(history['val_macro_f1']) if history['val_macro_f1'] else 0.0\n",
        "\n",
        "trainer.save_model(training_args.output_dir)\n",
        "tokenizer.save_pretrained(training_args.output_dir)\n",
        "\n",
        "import json\n",
        "with open(f\"{training_args.output_dir}/label_mappings.json\", \"w\") as f:\n",
        "    json.dump({\n",
        "        'id2label': id2label,\n",
        "        'label2id': label2id,\n",
        "        'id2stage': id2stage,\n",
        "        'stage2id': stage2id\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(f\"Training complete. Best F1: {best_val_f1:.4f}. Saved to {training_args.output_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "model_dir = \"./layer2_contextual_model\"\n",
        "\n",
        "with open(os.path.join(model_dir, 'label_mappings.json'), 'r') as f:\n",
        "    mappings = json.load(f)\n",
        "    id2label = {int(k): v for k, v in mappings['id2label'].items()}\n",
        "    label2id = mappings['label2id']\n",
        "    id2stage = {int(k): v for k, v in mappings['id2stage'].items()}\n",
        "    stage2id = mappings['stage2id']\n",
        "\n",
        "model = ContextAwareLayer2Classifier.from_pretrained(model_dir)\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "print(f\"Model loaded from {model_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "epochs_range = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "# Loss\n",
        "axes[0].plot(epochs_range, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
        "axes[0].plot(epochs_range, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Training and Validation Loss', fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy\n",
        "axes[1].plot(epochs_range, history['val_accuracy'], 'g-', linewidth=2)\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].set_title('Validation Accuracy', fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Macro F1\n",
        "axes[2].plot(epochs_range, history['val_macro_f1'], 'purple', linewidth=2)\n",
        "axes[2].set_xlabel('Epoch')\n",
        "axes[2].set_ylabel('Macro F1')\n",
        "axes[2].set_title('Validation Macro F1', fontweight='bold')\n",
        "axes[2].axhline(y=best_val_f1, color='red', linestyle='--', label=f'Best: {best_val_f1:.4f}')\n",
        "axes[2].legend()\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Training History', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_metrics, test_labels, test_preds, test_probs = evaluate_with_stages(\n",
        "    model, test_dataset, test_df, batch_size=EVAL_BATCH_SIZE\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"TEST SET RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nOverall Metrics:\")\n",
        "print(f\"  Accuracy: {test_metrics['accuracy']:.4f}\")\n",
        "print(f\"  Macro F1: {test_metrics['macro_f1']:.4f}\")\n",
        "print(f\"  Macro Precision: {test_metrics['macro_precision']:.4f}\")\n",
        "print(f\"  Macro Recall: {test_metrics['macro_recall']:.4f}\")\n",
        "print(f\"  Macro AUC: {test_metrics['macro_auc']:.4f}\")\n",
        "\n",
        "print(f\"\\nConfidence Statistics:\")\n",
        "print(f\"  Mean Confidence: {test_metrics['confidence_mean']:.4f}\")\n",
        "print(f\"  Confidence (Correct): {test_metrics['confidence_correct']:.4f}\")\n",
        "print(f\"  Confidence (Wrong): {test_metrics['confidence_wrong']:.4f}\")\n",
        "\n",
        "print(f\"\\nStratified by Stage:\")\n",
        "for stage_name in stage2id.keys():\n",
        "    acc = test_metrics[f'accuracy_stage_{stage_name}']\n",
        "    n = test_metrics[f'n_stage_{stage_name}']\n",
        "    print(f\"  {stage_name}: {acc:.4f} (n={n})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "class_names = [id2label[i] for i in range(NUM_LABELS)]\n",
        "x = np.arange(NUM_LABELS)\n",
        "bar_width = 0.6\n",
        "colors_intent = ['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4', '#ffeaa7', '#dfe6e9']\n",
        "\n",
        "# F1 Scores\n",
        "f1_scores = [test_metrics[f\"f1_{id2label[i]}\"] for i in range(NUM_LABELS)]\n",
        "bars = axes[0].bar(x, f1_scores, bar_width, color=colors_intent)\n",
        "axes[0].set_title('F1 Score per Class', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('F1 Score')\n",
        "axes[0].set_xticks(x)\n",
        "axes[0].set_xticklabels(class_names, rotation=45, ha='right', fontsize=9)\n",
        "axes[0].set_ylim(0, 1.15)\n",
        "axes[0].axhline(y=test_metrics[\"macro_f1\"], color='red', linestyle='--', linewidth=2, \n",
        "                label=f'Macro F1: {test_metrics[\"macro_f1\"]:.3f}')\n",
        "axes[0].legend(loc='upper right', framealpha=0.9)\n",
        "for bar, score in zip(bars, f1_scores):\n",
        "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, f'{score:.3f}', ha='center', fontsize=9)\n",
        "\n",
        "# Precision Scores\n",
        "precision_scores = [test_metrics[f\"precision_{id2label[i]}\"] for i in range(NUM_LABELS)]\n",
        "bars = axes[1].bar(x, precision_scores, bar_width, color=colors_intent)\n",
        "axes[1].set_title('Precision per Class', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylabel('Precision')\n",
        "axes[1].set_xticks(x)\n",
        "axes[1].set_xticklabels(class_names, rotation=45, ha='right', fontsize=9)\n",
        "axes[1].set_ylim(0, 1.15)\n",
        "axes[1].axhline(y=test_metrics[\"macro_precision\"], color='red', linestyle='--', linewidth=2, \n",
        "                label=f'Macro: {test_metrics[\"macro_precision\"]:.3f}')\n",
        "axes[1].legend(loc='upper right', framealpha=0.9)\n",
        "for bar, score in zip(bars, precision_scores):\n",
        "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, f'{score:.3f}', ha='center', fontsize=9)\n",
        "\n",
        "# Recall Scores\n",
        "recall_scores = [test_metrics[f\"recall_{id2label[i]}\"] for i in range(NUM_LABELS)]\n",
        "bars = axes[2].bar(x, recall_scores, bar_width, color=colors_intent)\n",
        "axes[2].set_title('Recall per Class', fontsize=12, fontweight='bold')\n",
        "axes[2].set_ylabel('Recall')\n",
        "axes[2].set_xticks(x)\n",
        "axes[2].set_xticklabels(class_names, rotation=45, ha='right', fontsize=9)\n",
        "axes[2].set_ylim(0, 1.15)\n",
        "axes[2].axhline(y=test_metrics[\"macro_recall\"], color='red', linestyle='--', linewidth=2, \n",
        "                label=f'Macro: {test_metrics[\"macro_recall\"]:.3f}')\n",
        "axes[2].legend(loc='upper right', framealpha=0.9)\n",
        "for bar, score in zip(bars, recall_scores):\n",
        "    axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, f'{score:.3f}', ha='center', fontsize=9)\n",
        "\n",
        "plt.suptitle('Per-Class Classification Metrics on Test Set', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Accuracy by Interview Stage\n",
        "stage_names = list(stage2id.keys())\n",
        "stage_accs = [test_metrics[f'accuracy_stage_{s}'] for s in stage_names]\n",
        "stage_counts = [test_metrics[f'n_stage_{s}'] for s in stage_names]\n",
        "colors_stage = ['#a29bfe', '#fd79a8', '#00b894', '#e17055']\n",
        "\n",
        "bars = ax.bar(range(len(stage_names)), stage_accs, color=colors_stage)\n",
        "ax.set_title('Accuracy by Interview Stage', fontsize=14, fontweight='bold')\n",
        "ax.set_ylabel('Accuracy', fontsize=12)\n",
        "ax.set_xlabel('Interview Stage', fontsize=12)\n",
        "ax.set_xticks(range(len(stage_names)))\n",
        "ax.set_xticklabels(stage_names, fontsize=11)\n",
        "ax.set_ylim(0, 1.15)\n",
        "ax.axhline(y=test_metrics['accuracy'], color='red', linestyle='--', linewidth=2,\n",
        "           label=f'Overall: {test_metrics[\"accuracy\"]:.3f}')\n",
        "ax.legend(loc='upper right', fontsize=11)\n",
        "\n",
        "for bar, acc, count in zip(bars, stage_accs, stage_counts):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
        "            f'{acc:.3f}\\n(n={count})', ha='center', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cm = confusion_matrix(test_labels, test_preds)\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "class_names = [id2label[i] for i in range(NUM_LABELS)]\n",
        "\n",
        "# Raw counts\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "axes[0].set_xlabel('Predicted')\n",
        "axes[0].set_ylabel('Actual')\n",
        "axes[0].set_title('Confusion Matrix (Counts)', fontweight='bold')\n",
        "\n",
        "# Normalized (percentages)\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', ax=axes[1],\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "axes[1].set_xlabel('Predicted')\n",
        "axes[1].set_ylabel('Actual')\n",
        "axes[1].set_title('Confusion Matrix (Normalized)', fontweight='bold')\n",
        "\n",
        "plt.suptitle('Test Set Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    user_query: str,\n",
        "    prev_agent_response: str = \"\",\n",
        "    interview_stage: str = \"opening\",\n",
        "    confidence_threshold: float = 0.5\n",
        "):\n",
        "    \"\"\"\n",
        "    Make a prediction with context.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained model\n",
        "        tokenizer: Tokenizer\n",
        "        user_query: Current user query\n",
        "        prev_agent_response: Previous agent/interviewer message\n",
        "        interview_stage: One of 'opening', 'technical_depth', 'challenge', 'closing'\n",
        "        confidence_threshold: Threshold for flagging low confidence\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with prediction results\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    combined_text = f\"{prev_agent_response} {tokenizer.sep_token} {user_query}\"\n",
        "    encoding = tokenizer(\n",
        "        combined_text,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding='max_length',\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    \n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    stage_tensor = torch.tensor([stage2id[interview_stage]], dtype=torch.long).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask, stage_tensor)\n",
        "        logits = outputs['logits']\n",
        "        probs = torch.softmax(logits, dim=-1)[0]\n",
        "    \n",
        "    pred_class = probs.argmax().item()\n",
        "    confidence = probs[pred_class].item()\n",
        "    pred_label = id2label[pred_class]\n",
        "    \n",
        "    if confidence >= confidence_threshold:\n",
        "        status = f\"ACCEPTED - {pred_label}\"\n",
        "    else:\n",
        "        status = f\"FLAGGED (low confidence) - Predicted: {pred_label}\"\n",
        "    \n",
        "    return {\n",
        "        'prediction': pred_label,\n",
        "        'prediction_id': pred_class,\n",
        "        'confidence': confidence,\n",
        "        'status': status,\n",
        "        'all_probs': {id2label[i]: probs[i].item() for i in range(NUM_LABELS)}\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_query = \"Do you guys have free snacks in the office?\"\n",
        "test_prev_response = \"Tell me about your most challenging technical project.\"\n",
        "test_stage = \"technical_depth\"\n",
        "\n",
        "result = predict(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    user_query=test_query,\n",
        "    prev_agent_response=test_prev_response,\n",
        "    interview_stage=test_stage\n",
        ")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"PREDICTION ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nInput:\")\n",
        "print(f\"  Query: '{test_query}'\")\n",
        "print(f\"  Prev Response: '{test_prev_response}'\")\n",
        "print(f\"  Stage: {test_stage}\")\n",
        "print(f\"\\nPrediction: {result['prediction']} (label {label2id[result['prediction']]})\")\n",
        "print(f\"Confidence: {result['confidence']:.1%}\")\n",
        "print(f\"Status: {result['status']}\")\n",
        "print(\"\\nTop 3 Predictions:\")\n",
        "sorted_probs = sorted(result['all_probs'].items(), key=lambda x: -x[1])\n",
        "for i, (label, prob) in enumerate(sorted_probs[:3], 1):\n",
        "    marker = \" <- PREDICTED\" if label == result['prediction'] else \"\"\n",
        "    print(f\"  {i}. {label:25s} {prob:6.1%}{marker}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model(model_dir: str, device: torch.device):\n",
        "    \"\"\"Load a saved context-aware model. Returns model, tokenizer, mappings.\"\"\"\n",
        "    import os\n",
        "    import json\n",
        "    \n",
        "    with open(os.path.join(model_dir, 'label_mappings.json'), 'r') as f:\n",
        "        mappings = json.load(f)\n",
        "    \n",
        "    model = ContextAwareLayer2Classifier.from_pretrained(model_dir)\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "    \n",
        "    return model, tokenizer, mappings"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
